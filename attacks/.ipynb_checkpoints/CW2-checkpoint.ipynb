{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Shamoyeeta/Adversarial-ML/blob/main/attacks/CW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GFtYfSMgJY2v",
    "outputId": "04633412-06f0-41b1-d81d-18571a9de35a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from timeit import default_timer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')           # noqa: E402\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyxtMwTDJmwt"
   },
   "outputs": [],
   "source": [
    "def cw(model, x, y=None, eps=1.0, ord_=2, T=2,\n",
    "       optimizer=tf.train.AdamOptimizer(learning_rate=0.1), alpha=0.9,\n",
    "       min_prob=0, clip=(0.0, 1.0)):\n",
    "    \"\"\"CarliniWagner (CW) attack.\n",
    "    Only CW-L2 and CW-Linf are implemented since I do not see the point of\n",
    "    embedding CW-L2 in CW-L1.  See https://arxiv.org/abs/1608.04644 for\n",
    "    details.\n",
    "    The idea of CW attack is to minimize a loss that comprises two parts: a)\n",
    "    the p-norm distance between the original image and the adversarial image,\n",
    "    and b) a term that encourages the incorrect classification of the\n",
    "    adversarial images.\n",
    "    Please note that CW is a optimization process, so it is tricky.  There are\n",
    "    lots of hyper-parameters to tune in order to get the best result.  The\n",
    "    binary search process for the best eps values is omitted here.  You could\n",
    "    do grid search to find the best parameter configuration, if you like.  I\n",
    "    demonstrate binary search for the best result in an example code.\n",
    "    :param model: The model wrapper.\n",
    "    :param x: The input clean sample, usually a placeholder.  NOTE that the\n",
    "              shape of x MUST be static, i.e., fixed when constructing the\n",
    "              graph.  This is because there are some variables that depends\n",
    "              upon this shape.\n",
    "    :param y: The target label.  Set to be the least-likely label when None.\n",
    "    :param eps: The scaling factor for the second penalty term.\n",
    "    :param ord_: The p-norm, 2 or inf.  Actually I only test whether it is 2\n",
    "        or not 2.\n",
    "    :param T: The temperature for sigmoid function.  In the original paper,\n",
    "              the author used (tanh(x)+1)/2 = sigmoid(2x), i.e., t=2.  During\n",
    "              our experiment, we found that this parameter also affects the\n",
    "              quality of generated adversarial samples.\n",
    "    :param optimizer: The optimizer used to minimize the CW loss.  Default to\n",
    "        be tf.AdamOptimizer with learning rate 0.1. Note the learning rate is\n",
    "        much larger than normal learning rate.\n",
    "    :param alpha: Used only in CW-L0.  The decreasing factor for the upper\n",
    "        bound of noise.\n",
    "    :param min_prob: The minimum confidence of adversarial examples.\n",
    "        Generally larger min_prob wil lresult in more noise.\n",
    "    :param clip: A tuple (clip_min, clip_max), which denotes the range of\n",
    "        values in x.\n",
    "    :return: A tuple (train_op, xadv, noise).  Run train_op for some epochs to\n",
    "             generate the adversarial image, then run xadv to get the final\n",
    "             adversarial image.  Noise is in the sigmoid-space instead of the\n",
    "             input space.  It is returned because we need to clear noise\n",
    "             before each batched attacks.\n",
    "    \"\"\"\n",
    "    xshape = x.get_shape().as_list()\n",
    "    noise = tf.get_variable('noise', xshape, tf.float32,\n",
    "                            initializer=tf.initializers.zeros)\n",
    "\n",
    "    # scale input to (0, 1)\n",
    "    x_scaled = (x - clip[0]) / (clip[1] - clip[0])\n",
    "\n",
    "    # change to sigmoid-space, clip to avoid overflow.\n",
    "    z = tf.clip_by_value(x_scaled, 1e-8, 1-1e-8)\n",
    "    xinv = tf.log(z / (1 - z)) / T\n",
    "\n",
    "    # add noise in sigmoid-space and map back to input domain\n",
    "    xadv = tf.sigmoid(T * (xinv + noise))\n",
    "    xadv = xadv * (clip[1] - clip[0]) + clip[0]\n",
    "\n",
    "    ybar, logits = model(xadv, logits=True)\n",
    "    ydim = ybar.get_shape().as_list()[1]\n",
    "\n",
    "    if y is not None:\n",
    "        y = tf.cond(tf.equal(tf.rank(y), 0),\n",
    "                    lambda: tf.fill([xshape[0]], y),\n",
    "                    lambda: tf.identity(y))\n",
    "    else:\n",
    "        # we set target to the least-likely label\n",
    "        y = tf.argmin(ybar, axis=1, output_type=tf.int32)\n",
    "\n",
    "    mask = tf.one_hot(y, ydim, on_value=0.0, off_value=float('inf'))\n",
    "    yt = tf.reduce_max(logits - mask, axis=1)\n",
    "    yo = tf.reduce_max(logits, axis=1)\n",
    "\n",
    "    # encourage to classify to a wrong category\n",
    "    loss0 = tf.nn.relu(yo - yt + min_prob)\n",
    "\n",
    "    axis = list(range(1, len(xshape)))\n",
    "    ord_ = float(ord_)\n",
    "\n",
    "    # make sure the adversarial images are visually close\n",
    "    if 2 == ord_:\n",
    "        # CW-L2 Original paper uses the reduce_sum version.  These two\n",
    "        # implementation does not differ much.\n",
    "\n",
    "        # loss1 = tf.reduce_sum(tf.square(xadv-x), axis=axis)\n",
    "        loss1 = tf.reduce_mean(tf.square(xadv-x))\n",
    "    else:\n",
    "        # CW-Linf\n",
    "        tau0 = tf.fill([xshape[0]] + [1]*len(axis), clip[1])\n",
    "        tau = tf.get_variable('cw8-noise-upperbound', dtype=tf.float32,\n",
    "                              initializer=tau0, trainable=False)\n",
    "        diff = xadv - x - tau\n",
    "\n",
    "        # if all values are smaller than the upper bound value tau, we reduce\n",
    "        # this value via tau*0.9 to make sure L-inf does not get stuck.\n",
    "        tau = alpha * tf.to_float(tf.reduce_all(diff < 0, axis=axis))\n",
    "        loss1 = tf.nn.relu(tf.reduce_sum(diff, axis=axis))\n",
    "\n",
    "    loss = eps*loss0 + loss1\n",
    "    train_op = optimizer.minimize(loss, var_list=[noise])\n",
    "\n",
    "    # We may need to update tau after each iteration.  Refer to the CW-Linf\n",
    "    # section in the original paper.\n",
    "    if 2 != ord_:\n",
    "        train_op = tf.group(train_op, tau)\n",
    "\n",
    "    return train_op, xadv, noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jUZ3EQ8EJsAG"
   },
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "\n",
    "img_size = 28\n",
    "img_chan = 1\n",
    "n_classes = 10\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TtJWBd-_Jvr_"
   },
   "outputs": [],
   "source": [
    "class Timer(object):\n",
    "    def __init__(self, msg='Starting.....', timer=default_timer, factor=1,\n",
    "                 fmt=\"------- elapsed {:.4f}s --------\"):\n",
    "        self.timer = timer\n",
    "        self.factor = factor\n",
    "        self.fmt = fmt\n",
    "        self.end = None\n",
    "        self.msg = msg\n",
    "\n",
    "    def __call__(self):\n",
    "        \"\"\"\n",
    "        Return the current time\n",
    "        \"\"\"\n",
    "        return self.timer()\n",
    "\n",
    "    def __enter__(self):\n",
    "        \"\"\"\n",
    "        Set the start time\n",
    "        \"\"\"\n",
    "        print(self.msg)\n",
    "        self.start = self()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, exc_traceback):\n",
    "        \"\"\"\n",
    "        Set the end time\n",
    "        \"\"\"\n",
    "        self.end = self()\n",
    "        print(str(self))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.fmt.format(self.elapsed)\n",
    "\n",
    "    @property\n",
    "    def elapsed(self):\n",
    "        if self.end is None:\n",
    "            # if elapsed is called in the context manager scope\n",
    "            return (self() - self.start) * self.factor\n",
    "        else:\n",
    "            # if elapsed is called out of the context manager scope\n",
    "            return (self.end - self.start) * self.factor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oZAvY7oiJy1Q",
    "outputId": "f62d7b1c-5582-42ab-d9a4-143a996830de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading MNIST\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "print('\\nLoading MNIST')\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = np.reshape(X_train, [-1, img_size, img_size, img_chan])\n",
    "X_train = X_train.astype(np.float32) / 255\n",
    "X_test = np.reshape(X_test, [-1, img_size, img_size, img_chan])\n",
    "X_test = X_test.astype(np.float32) / 255\n",
    "\n",
    "to_categorical = tf.keras.utils.to_categorical\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q_JDYAkwJ00B",
    "outputId": "25296284-67e2-4850-99dd-44ad2ce3fb75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Spliting data\n"
     ]
    }
   ],
   "source": [
    "print('\\nSpliting data')\n",
    "\n",
    "ind = np.random.permutation(X_train.shape[0])\n",
    "X_train, y_train = X_train[ind], y_train[ind]\n",
    "\n",
    "VALIDATION_SPLIT = 0.1\n",
    "n = int(X_train.shape[0] * (1-VALIDATION_SPLIT))\n",
    "X_valid = X_train[n:]\n",
    "X_train = X_train[:n]\n",
    "y_valid = y_train[n:]\n",
    "y_train = y_train[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MmaRz9-lJ3bG",
    "outputId": "b7fd4582-47a4-4d14-c349-599528621de2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Construction graph\n"
     ]
    }
   ],
   "source": [
    "print('\\nConstruction graph')\n",
    "\n",
    "\n",
    "def model(x, logits=False, training=False):\n",
    "    with tf.variable_scope('conv0'):\n",
    "        z = tf.layers.conv2d(x, filters=32, kernel_size=[3, 3],\n",
    "                             padding='same', activation=tf.nn.relu)\n",
    "        z = tf.layers.max_pooling2d(z, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    with tf.variable_scope('conv1'):\n",
    "        z = tf.layers.conv2d(z, filters=64, kernel_size=[3, 3],\n",
    "                             padding='same', activation=tf.nn.relu)\n",
    "        z = tf.layers.max_pooling2d(z, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    with tf.variable_scope('flatten'):\n",
    "        shape = z.get_shape().as_list()\n",
    "        z = tf.reshape(z, [-1, np.prod(shape[1:])])\n",
    "\n",
    "    with tf.variable_scope('mlp'):\n",
    "        z = tf.layers.dense(z, units=128, activation=tf.nn.relu)\n",
    "        z = tf.layers.dropout(z, rate=0.25, training=training)\n",
    "\n",
    "    logits_ = tf.layers.dense(z, units=10, name='logits')\n",
    "    y = tf.nn.softmax(logits_, name='ybar')\n",
    "\n",
    "    if logits:\n",
    "        return y, logits_\n",
    "    return y\n",
    "\n",
    "\n",
    "class Dummy:\n",
    "    pass\n",
    "\n",
    "\n",
    "env = Dummy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tlLrYOZJJ66n",
    "outputId": "950117ec-0f26-4266-d3a2-41f5d9d74562"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
      "  import sys\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: UserWarning: `tf.layers.max_pooling2d` is deprecated and will be removed in a future version. Please use `tf.keras.layers.MaxPooling2D` instead.\n",
      "  \n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
      "  if sys.path[0] == '':\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: UserWarning: `tf.layers.max_pooling2d` is deprecated and will be removed in a future version. Please use `tf.keras.layers.MaxPooling2D` instead.\n",
      "  del sys.path[0]\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing graph\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('model', reuse=tf.AUTO_REUSE):\n",
    "    env.x = tf.placeholder(tf.float32, (None, img_size, img_size, img_chan),\n",
    "                           name='x')\n",
    "    env.y = tf.placeholder(tf.float32, (None, n_classes), name='y')\n",
    "    env.training = tf.placeholder_with_default(False, (), name='mode')\n",
    "\n",
    "    env.ybar, logits = model(env.x, logits=True, training=env.training)\n",
    "\n",
    "    with tf.variable_scope('acc'):\n",
    "        count = tf.equal(tf.argmax(env.y, axis=1), tf.argmax(env.ybar, axis=1))\n",
    "        env.acc = tf.reduce_mean(tf.cast(count, tf.float32), name='acc')\n",
    "\n",
    "    with tf.variable_scope('loss'):\n",
    "        xent = tf.nn.softmax_cross_entropy_with_logits(labels=env.y,\n",
    "                                                       logits=logits)\n",
    "        env.loss = tf.reduce_mean(xent, name='loss')\n",
    "\n",
    "    with tf.variable_scope('train_op'):\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        vs = tf.global_variables()\n",
    "        env.train_op = optimizer.minimize(env.loss, var_list=vs)\n",
    "\n",
    "    env.saver = tf.train.Saver()\n",
    "\n",
    "    # Note here that the shape has to be fixed during the graph construction\n",
    "    # since the internal variable depends upon the shape.\n",
    "    env.x_fixed = tf.placeholder(\n",
    "        tf.float32, (batch_size, img_size, img_size, img_chan),\n",
    "        name='x_fixed')\n",
    "    env.adv_eps = tf.placeholder(tf.float32, (), name='adv_eps')\n",
    "    env.adv_y = tf.placeholder(tf.int32, (), name='adv_y')\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.1)\n",
    "    env.adv_train_op, env.xadv, env.noise = cw(model, env.x_fixed, ord_='inf',\n",
    "                                               y=env.adv_y, eps=env.adv_eps,\n",
    "                                               optimizer=optimizer)\n",
    "\n",
    "print('\\nInitializing graph')\n",
    "\n",
    "env.sess = tf.InteractiveSession()\n",
    "env.sess.run(tf.global_variables_initializer())\n",
    "env.sess.run(tf.local_variables_initializer())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2bGUII7LJ9Wp"
   },
   "outputs": [],
   "source": [
    "def evaluate(env, X_data, y_data, batch_size=128):\n",
    "    \"\"\"\n",
    "    Evaluate TF model by running env.loss and env.acc.\n",
    "    \"\"\"\n",
    "    print('\\nEvaluating')\n",
    "\n",
    "    n_sample = X_data.shape[0]\n",
    "    n_batch = int((n_sample+batch_size-1) / batch_size)\n",
    "    loss, acc = 0, 0\n",
    "\n",
    "    for batch in range(n_batch):\n",
    "        print(' batch {0}/{1}'.format(batch + 1, n_batch), end='\\r')\n",
    "        start = batch * batch_size\n",
    "        end = min(n_sample, start + batch_size)\n",
    "        cnt = end - start\n",
    "        batch_loss, batch_acc = env.sess.run(\n",
    "            [env.loss, env.acc],\n",
    "            feed_dict={env.x: X_data[start:end],\n",
    "                       env.y: y_data[start:end]})\n",
    "        loss += batch_loss * cnt\n",
    "        acc += batch_acc * cnt\n",
    "    loss /= n_sample\n",
    "    acc /= n_sample\n",
    "\n",
    "    print(' loss: {0:.4f} acc: {1:.4f}'.format(loss, acc))\n",
    "    return loss, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GtlRqZr8J_yl"
   },
   "outputs": [],
   "source": [
    "def train(env, X_data, y_data, X_valid=None, y_valid=None, epochs=1,\n",
    "          load=False, shuffle=True, batch_size=128, name='model'):\n",
    "    \"\"\"\n",
    "    Train a TF model by running env.train_op.\n",
    "    \"\"\"\n",
    "    if load:\n",
    "        if not hasattr(env, 'saver'):\n",
    "            return print('\\nError: cannot find saver op')\n",
    "        print('\\nLoading saved model')\n",
    "        return env.saver.restore(env.sess, 'model/{}'.format(name))\n",
    "\n",
    "    print('\\nTrain model')\n",
    "    n_sample = X_data.shape[0]\n",
    "    n_batch = int((n_sample+batch_size-1) / batch_size)\n",
    "    for epoch in range(epochs):\n",
    "        print('\\nEpoch {0}/{1}'.format(epoch + 1, epochs))\n",
    "\n",
    "        if shuffle:\n",
    "            print('\\nShuffling data')\n",
    "            ind = np.arange(n_sample)\n",
    "            np.random.shuffle(ind)\n",
    "            X_data = X_data[ind]\n",
    "            y_data = y_data[ind]\n",
    "\n",
    "        for batch in range(n_batch):\n",
    "            print(' batch {0}/{1}'.format(batch + 1, n_batch), end='\\r')\n",
    "            start = batch * batch_size\n",
    "            end = min(n_sample, start + batch_size)\n",
    "            env.sess.run(env.train_op, feed_dict={env.x: X_data[start:end],\n",
    "                                                  env.y: y_data[start:end],\n",
    "                                                  env.training: True})\n",
    "        if X_valid is not None:\n",
    "            evaluate(env, X_valid, y_valid)\n",
    "\n",
    "    if hasattr(env, 'saver'):\n",
    "        print('\\n Saving model')\n",
    "        os.makedirs('model', exist_ok=True)\n",
    "        env.saver.save(env.sess, 'model/{}'.format(name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LdMUVNWLKDv1"
   },
   "outputs": [],
   "source": [
    "def predict(env, X_data, batch_size=128):\n",
    "    \"\"\"\n",
    "    Do inference by running env.ybar.\n",
    "    \"\"\"\n",
    "    print('\\nPredicting')\n",
    "    n_classes = env.ybar.get_shape().as_list()[1]\n",
    "\n",
    "    n_sample = X_data.shape[0]\n",
    "    n_batch = int((n_sample+batch_size-1) / batch_size)\n",
    "    yval = np.empty((n_sample, n_classes))\n",
    "\n",
    "    for batch in range(n_batch):\n",
    "        print(' batch {0}/{1}'.format(batch + 1, n_batch), end='\\r')\n",
    "        start = batch * batch_size\n",
    "        end = min(n_sample, start + batch_size)\n",
    "        y_batch = env.sess.run(env.ybar, feed_dict={env.x: X_data[start:end]})\n",
    "        yval[start:end] = y_batch\n",
    "    print()\n",
    "    return yval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sme2U6kQKGE-"
   },
   "outputs": [],
   "source": [
    "def make_cw(env, X_data, epochs=1, eps=0.1, batch_size=batch_size):\n",
    "    \"\"\"\n",
    "    Generate adversarial via CW optimization.\n",
    "    \"\"\"\n",
    "    print('\\nMaking adversarials via CW')\n",
    "\n",
    "    n_sample = X_data.shape[0]\n",
    "    n_batch = int((n_sample + batch_size - 1) / batch_size)\n",
    "    X_adv = np.empty_like(X_data)\n",
    "\n",
    "\n",
    "    # for batch in range(n_batch):\n",
    "    for batch in range(n_batch):\n",
    "        with Timer('Batch {0}/{1}   '.format(batch + 1, n_batch)):\n",
    "            end = min(n_sample, (batch+1) * batch_size)\n",
    "            start = end - batch_size\n",
    "            feed_dict = {\n",
    "                env.x_fixed: X_data[start:end],\n",
    "                env.adv_eps: eps,\n",
    "                # env.adv_y: np.random.choice(n_classes)\n",
    "                env.adv_y:5\n",
    "                }\n",
    "\n",
    "            env.sess.run(env.noise.initializer)\n",
    "            for epoch in range(epochs):\n",
    "                env.sess.run(env.adv_train_op, feed_dict=feed_dict)\n",
    "\n",
    "            xadv = env.sess.run(env.xadv, feed_dict=feed_dict)\n",
    "            X_adv[start:end] = xadv\n",
    "\n",
    "    return X_adv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E3ARcA05KJj2",
    "outputId": "67fec359-1c9b-4427-9382-f06893af3eb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training\n",
      "\n",
      "Train model\n",
      "\n",
      "Epoch 1/5\n",
      "\n",
      "Shuffling data\n",
      "\n",
      "Evaluating\n",
      " loss: 0.0705 acc: 0.9775\n",
      "\n",
      "Epoch 2/5\n",
      "\n",
      "Shuffling data\n",
      " batch 422/422\n",
      "Evaluating\n",
      " loss: 0.0399 acc: 0.9885\n",
      "\n",
      "Epoch 3/5\n",
      "\n",
      "Shuffling data\n",
      " batch 422/422\n",
      "Evaluating\n",
      " loss: 0.0338 acc: 0.9893\n",
      "\n",
      "Epoch 4/5\n",
      "\n",
      "Shuffling data\n",
      " batch 422/422\n",
      "Evaluating\n",
      " loss: 0.0311 acc: 0.9900\n",
      "\n",
      "Epoch 5/5\n",
      "\n",
      "Shuffling data\n",
      " batch 422/422\n",
      "Evaluating\n",
      " loss: 0.0318 acc: 0.9900\n",
      "\n",
      " Saving model\n",
      "\n",
      "Evaluating on clean data\n",
      "\n",
      "Evaluating\n",
      " loss: 0.0287 acc: 0.9908\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.028674748412892224, 0.9908)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('\\nTraining')\n",
    "\n",
    "train(env, X_train, y_train, X_valid, y_valid, load=False, epochs=5,name='mnist')\n",
    "\n",
    "print('\\nEvaluating on clean data')\n",
    "\n",
    "evaluate(env, X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JMQ4GISOBb2Q",
    "outputId": "90f12d00-afa7-4a6c-c168-96baf1effdc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating adversarial data\n",
      "\n",
      "Making adversarials via CW\n",
      "Batch 1/2   \n",
      "------- elapsed 0.4521s --------\n",
      "Batch 2/2   \n",
      "------- elapsed 0.3126s --------\n"
     ]
    }
   ],
   "source": [
    "print('\\nGenerating adversarial data')\n",
    "\n",
    "# It takes a while to run through the full dataset, thus, we demo the result\n",
    "# through a smaller dataset.  We could actually find the best parameter\n",
    "# configuration on a smaller dataset, and then apply to the full dataset.\n",
    "n_sample = 128\n",
    "ind = np.random.choice(X_test.shape[0], size=n_sample)\n",
    "X_test = X_test[ind]\n",
    "y_test = y_test[ind]\n",
    "\n",
    "X_adv = make_cw(env, X_test, eps=1, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ou8l5e9HkRDQ",
    "outputId": "c42e1ec3-901e-4d2e-a671-9c0ee99ead10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on adversarial data\n",
      "\n",
      "Evaluating\n",
      " batch 1/1\r",
      " loss: 6.1381 acc: 0.0703\n",
      "\n",
      "Randomly sample adversarial data from each category\n",
      "\n",
      "Predicting\n",
      " batch 1/1\r\n",
      "\n",
      "Predicting\n",
      " batch 1/1\r\n"
     ]
    }
   ],
   "source": [
    "print('\\nEvaluating on adversarial data')\n",
    "\n",
    "evaluate(env, X_adv, y_test)\n",
    "\n",
    "print('\\nRandomly sample adversarial data from each category')\n",
    "\n",
    "y1 = predict(env, X_test)\n",
    "y2 = predict(env, X_adv)\n",
    "\n",
    "z0 = np.argmax(y_test, axis=1)\n",
    "z1 = np.argmax(y1, axis=1)\n",
    "z2 = np.argmax(y2, axis=1)\n",
    "\n",
    "ind = np.logical_and(z0 == z1, z1 != z2)\n",
    "# print('success: ', np.sum(ind))\n",
    "\n",
    "ind = z0 == z1\n",
    "\n",
    "#To save in pickle file\n",
    "X_adv_org = X_adv\n",
    "label_org = z1\n",
    "\n",
    "X_test = X_test[ind]\n",
    "X_adv = X_adv[ind]\n",
    "z1 = z1[ind]\n",
    "z2 = z2[ind]\n",
    "y2 = y2[ind]\n",
    "\n",
    "ind, = np.where(z1 != z2)\n",
    "cur = np.random.choice(ind, size=n_classes)\n",
    "X_org = np.squeeze(X_test[cur])\n",
    "X_tmp = np.squeeze(X_adv[cur])\n",
    "y_tmp = y2[cur]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-p2twGNpenYL",
    "outputId": "be51098a-7d23-4fc6-e14a-f97dbf2b8954"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "128\n",
      "127\n",
      "128\n",
      "127\n"
     ]
    }
   ],
   "source": [
    "print(len(X_adv_org))\n",
    "print(len(label_org))\n",
    "print(len(X_adv))\n",
    "print(len(z0))\n",
    "print(len(z1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FBZe72umkTOs",
    "outputId": "7194383e-696c-4ff5-a32c-80ea49c77f86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving figure\n"
     ]
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(10, 2.2))\n",
    "gs = gridspec.GridSpec(2, 10, wspace=0.05, hspace=0.05)\n",
    "\n",
    "label = np.argmax(y_tmp, axis=1)\n",
    "proba = np.max(y_tmp, axis=1)\n",
    "for i in range(10):\n",
    "  ax = fig.add_subplot(gs[0, i])\n",
    "  ax.imshow(X_org[i], cmap='gray', interpolation='none')\n",
    "  ax.set_xticks([])\n",
    "  ax.set_yticks([])\n",
    "\n",
    "  ax = fig.add_subplot(gs[1, i])\n",
    "  ax.imshow(X_tmp[i], cmap='gray', interpolation='none')\n",
    "  ax.set_xticks([])\n",
    "  ax.set_yticks([])\n",
    "\n",
    "  ax.set_xlabel('{0} ({1:.2f})'.format(label[i], proba[i]), fontsize=12)\n",
    "\n",
    "print('\\nSaving figure')\n",
    "\n",
    "gs.tight_layout(fig)\n",
    "os.makedirs('img/', exist_ok=True)\n",
    "plt.savefig('img/cw8_mnist.png')\n",
    "# from IPython.display import Image\n",
    "# Image('img/cw8_mnist.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## @author vinoy\n",
    "## @date 17-11-22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RRa4R2_qbP2G"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "db = {}\n",
    "db['x_adv'] = X_adv\n",
    "db['label'] = z1\n",
    "db['x_org'] = X_org\n",
    "db['prob'] = proba\n",
    "\n",
    "# Its important to use binary mode\n",
    "dbfile = open('images.pkl', 'ab')\n",
    "      \n",
    "# source, destination\n",
    "pickle.dump(db, dbfile)                     \n",
    "dbfile.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
